<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.5.0 --> <title>PPCA Tutorial | MXFusion</title> <meta name="generator" content="Jekyll v3.8.5" /> <meta property="og:title" content="PPCA Tutorial" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Probabalistic Modeling Introduction" /> <meta property="og:description" content="Probabalistic Modeling Introduction" /> <link rel="canonical" href="http://localhost:4000/mxfusion/ppca_tutorial/" /> <meta property="og:url" content="http://localhost:4000/mxfusion/ppca_tutorial/" /> <meta property="og:site_name" content="MXFusion" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2018-08-01T12:35:34+01:00" /> <script type="application/ld+json"> {"url":"http://localhost:4000/mxfusion/ppca_tutorial/","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/mxfusion/ppca_tutorial/"},"description":"Probabalistic Modeling Introduction","headline":"PPCA Tutorial","dateModified":"2018-08-01T12:35:34+01:00","datePublished":"2018-08-01T12:35:34+01:00","@type":"BlogPosting","@context":"http://schema.org"}</script> <!-- End Jekyll SEO tag --> <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"> --> <link rel="stylesheet" href="/mxfusion/css/main.css"> <link rel="alternate" type="application/rss+xml" title="MXFusion" href="http://localhost:4000/mxfusion/feed.xml"> <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> </head> <body> <header class="site-header"> <nav class="navbar navbar-default"> <div class="container-fluid"> <!-- Brand and toggle get grouped for better mobile display --> <div class="navbar-header"> <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/mxfusion/">MXFusion</a> </div> <!-- Collect the nav links, forms, and other content for toggling --> <div class="collapse navbar-collapse " id="bs-example-navbar-collapse-1"> <ul class="nav navbar-nav navbar-right"> <li><a href="/mxfusion/about/">About</a></li> <li><a href="http://nbviewer.jupyter.org/github/amzn/mxfusion/blob/master/notebooks/index.ipynb">Tutorials</a></li> <li><a href="https://github.com/amzn/mxfusion/tree/master/ xfusion/examples">Examples</a></li> <li class="dropdown"> <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Resources<span class="caret"></span></a> <ul class="dropdown-menu"> <li><a href="/mxfusion/installation/">Installation</a></li> <li role="separator" class="divider"></li> <li><a href="https://mxfusion.readthedocs.io/en/latest/">Documentation</a></li> <li><a href="https://mxfusion.readthedocs.io/en/latest/api/mxfusion.html">API reference</a></li> <li role="separator" class="divider"></li> <li><a href="/mxfusion/citation/">Citation</a></li> <li><a href="/mxfusion/news/">News</a></li> </ul> <li><a href="https://github.com/amzn/mxfusion">Github</a></li> </li> </ul> </div><!-- /.navbar-collapse --> </div><!-- /.container-fluid --> </nav> </header> <div class="container"> <div class="wrapper"> <div class="row"> <div class="col-md-8"> <article class="post" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <h1 class="post-title" itemprop="name headline">PPCA Tutorial</h1> <p class="post-meta"></p> </header> <div class="post-content" itemprop="articleBody"> <h2 id="probabalistic-modeling-introduction">Probabalistic Modeling Introduction</h2> <p>Probabilistic Models can be categorized into directed graphical models (DGM, Bayes Net) and undirected graphical models (UGM). Most popular probabilistic models are DGMs, so MXFusion will only support the definition of DGMs unless there is a strong customer need of UGMs in future.</p> <p>A DGM can be fully defined using 3 basic components: deterministic functions, probabilistic distributions, and random variables. We show the interface for defining a model using each of the three components below.</p> <p>First lets import the basic libraries we’ll need to train our model and visualize some data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">mxfusion</span> <span class="k">as</span> <span class="n">mf</span>
<span class="kn">import</span> <span class="nn">mxnet</span> <span class="k">as</span> <span class="n">mx</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div> <h2 id="data-generation">Data Generation</h2> <p>We’ll take as our function to learn components of the <a href="https://en.wikipedia.org/wiki/Logarithmic_spiral">log spiral function</a> because it’s 2-dimensional and easy to visualize.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_spiral</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">t</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</code></pre></div></div> <p>We parameterize the function with 100 data points and plot the resulting function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">D</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">a</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">log_spiral</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">r</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(100, 2)
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">r</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">r</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s">'.'</span><span class="p">)</span>
</code></pre></div></div> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1a1cbf4d68&gt;]
</code></pre></div></div> <p><img src="/mxfusion/images/ppca_tutorial_10_1.png" alt="png" /></p> <p>We now project our $K$ dimensional <code class="highlighter-rouge">r</code> into a high-dimensional $D$ space using a random matrix of random weights $W$. Now that <code class="highlighter-rouge">r</code> is embedded in a $D$ dimensional space the goal of PPCA will be to recover <code class="highlighter-rouge">r</code> in it’s original low-dimensional $K$ space.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-3</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># from sklearn.decomposition import PCA
# pca = PCA(n_components=2)
# new_r = pca.fit_transform(x_train)
# plt.plot(new_r[:,0], new_r[:,1],'.')
</span></code></pre></div></div> <p>You can explore the higher dimensional data manually by changing <code class="highlighter-rouge">dim1</code> and <code class="highlighter-rouge">dim2</code> in the following cell.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dim1</span> <span class="o">=</span> <span class="mi">79</span>
<span class="n">dim2</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">[:,</span><span class="n">dim1</span><span class="p">],</span> <span class="n">x_train</span><span class="p">[:,</span><span class="n">dim2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'blue'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Simulated data set"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <p><img src="/mxfusion/images/ppca_tutorial_15_0.png" alt="png" /></p> <h2 id="mxfusion-model-definition">MXFusion Model Definition</h2> <p>Import MXFusion and MXNet modelling components</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mxfusion.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">import</span> <span class="nn">mxnet.gluon.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mxfusion.components</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">mxfusion.components.variables</span> <span class="kn">import</span> <span class="n">PositiveTransformation</span>
<span class="kn">from</span> <span class="nn">mxfusion.components.functions.operators</span> <span class="kn">import</span> <span class="n">broadcast_to</span>
</code></pre></div></div> <p>The primary data structure in MXFusion is the Model. Models hold ModelComponents, such as Variables, Distributions, and Functions which are the what define a probabilistic model.</p> <p>The model we’ll be defining for PPCA is:</p> <p>$p(z)$ ~ $N(\mathbf{\mu}, \mathbf{\Sigma)}$</p> <table> <tbody> <tr> <td>$p(x</td> <td>z,\theta)$ ~ $N(\mathbf{Wz} + \mu, \Psi)$</td> </tr> </tbody> </table> <p>where:</p> <p>$z \in \mathbb{R}^{N x K}, \mathbf{\mu} \in \mathbb{R}^K, \mathbf{\Sigma} \in \mathbb{R}^{NxKxK}, x \in \mathbb{R}^{NxD}$</p> <p>$\Psi \in \mathbb{R}^{NxDxD}, \Psi = [\Psi_0, \dots, \Psi_N], \Psi_i = \sigma^2\mathbf{I}$</p> <p>$z$ here is our latent variable of interest, $x$ is the observed data, and all other variables are parameters or constants of the model.</p> <p>First we create an MXFusion Model object to build our PPCA model on.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
</code></pre></div></div> <p>We attach <code class="highlighter-rouge">Variable</code> objects to our model to collect them in a centralized place. Internally, these are organized into a factor graph which is used during Inference.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">D</span><span class="p">),</span> <span class="n">initial_value</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">D</span><span class="p">)))</span>
</code></pre></div></div> <p>Because the mean of $x$’s distribution is composed of the dot product of $z$ and $W$, we need to create a dot product function. First we create a dot product function in MXNet and then wrap the function into MXFusion using the MXFusionGluonFunction class. <code class="highlighter-rouge">m.dot</code> can then be called like a normal python function and will apply to the variables it is called on.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dot</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HybridLambda</span><span class="p">(</span><span class="n">function</span><span class="o">=</span><span class="s">'dot'</span><span class="p">)</span>
<span class="n">m</span><span class="o">.</span><span class="n">dot</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">MXFusionGluonFunction</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">broadcastable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>Now we define <code class="highlighter-rouge">m.z</code> which has an identity matrix covariance <code class="highlighter-rouge">cov</code> and zero mean.</p> <p><code class="highlighter-rouge">m.z</code> and <code class="highlighter-rouge">sigma_2</code> are then used to define <code class="highlighter-rouge">m.x</code>.</p> <p>Note that both <code class="highlighter-rouge">sigma_2</code> and <code class="highlighter-rouge">cov</code> will be added implicitly into the <code class="highlighter-rouge">Model</code> because they are inputs to <code class="highlighter-rouge">m.x</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cov</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">)),</span> <span class="mi">0</span><span class="p">),</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>
<span class="n">m</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="o">.</span><span class="n">define_variable</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="p">)),</span> <span class="n">covariance</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>
<span class="n">m</span><span class="o">.</span><span class="n">sigma_2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">transformation</span><span class="o">=</span><span class="n">PositiveTransformation</span><span class="p">())</span>
<span class="n">m</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="o">.</span><span class="n">define_variable</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">z</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">),</span> <span class="n">variance</span><span class="o">=</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">sigma_2</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">)),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span>
</code></pre></div></div> <h2 id="posterior-definition">Posterior Definition</h2> <p>Now that we have our model, we need to define a posterior with parameters for the inference algorithm to optimize. When constructing a Posterior, we pass in the Model it is defined over and ModelComponent’s from the original Model are accessible and visible in the Posterior.</p> <p>The covariance matrix must continue to be positive definite throughout the optimization process in order to succeed in the Cholesky decomposition when drawing samples or computing the log pdf of <code class="highlighter-rouge">q.z</code>. To satisfy this, we pass the covariance matrix parameters through a Gluon function that forces it into a Symmetric matrix for which suitable initialization values should maintain positive definite-ness throughout the optimization procedure.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mxfusion.inference</span> <span class="kn">import</span> <span class="n">BatchInferenceLoop</span><span class="p">,</span> <span class="n">GradBasedInference</span><span class="p">,</span> <span class="n">StochasticVariationalInference</span>
<span class="k">class</span> <span class="nc">SymmetricMatrix</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">gluon</span><span class="o">.</span><span class="n">HybridBlock</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">hybrid_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="nb">sum</span><span class="p">((</span><span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div> <p>While this model has an analytical solution, we will run Variational Inference to find the posterior to demonstrate inference in a setting where the answer is known.</p> <p>We place a multivariate normal prior over $z$ because that is $z$’s prior in the model and we don’t need to approximate anything in this case. Because the form we’re optimizing over is the true model, the optimization is convex and will always converge to the same answer given by classical PCA given enough iterations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Posterior</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">sym</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">components</span><span class="o">.</span><span class="n">functions</span><span class="o">.</span><span class="n">MXFusionGluonFunction</span><span class="p">(</span><span class="n">SymmetricMatrix</span><span class="p">(),</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">broadcastable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">),</span> <span class="n">initial_value</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1e-2</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">)))</span>
<span class="n">q</span><span class="o">.</span><span class="n">post_cov</span> <span class="o">=</span> <span class="n">sym</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
<span class="n">q</span><span class="o">.</span><span class="n">post_mean</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="p">),</span> <span class="n">initial_value</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">K</span><span class="p">)))</span>
<span class="n">q</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">set_prior</span><span class="p">(</span><span class="n">mf</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">MultivariateNormal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">post_mean</span><span class="p">,</span> <span class="n">covariance</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">post_cov</span><span class="p">))</span>
</code></pre></div></div> <p>We now take our posterior and model, along with an observation pattern (in our case only <code class="highlighter-rouge">m.x</code> is observed) and create an inference algorithm. This inference algorithm is combined with a gradient loop to create the Inference method <code class="highlighter-rouge">infr</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">observed</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">x</span><span class="p">]</span>
<span class="n">alg</span> <span class="o">=</span> <span class="n">StochasticVariationalInference</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">posterior</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>
<span class="n">infr</span> <span class="o">=</span> <span class="n">GradBasedInference</span><span class="p">(</span><span class="n">inference_algorithm</span><span class="o">=</span><span class="n">alg</span><span class="p">,</span>  <span class="n">grad_loop</span><span class="o">=</span><span class="n">BatchInferenceLoop</span><span class="p">())</span>
</code></pre></div></div> <p>The inference method is then initialized with our training data and we run optimiziation for a while until convergence.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">infr</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">infr</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
</code></pre></div></div> <p>Once training completes, we retrieve the posterior mean (our trained representation for $\mathbf{Wz} + \mu$) from the inference method and plot it. As shown, the plot recovers (up to rotation) the original 2D data quite well.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">post_z_mean</span> <span class="o">=</span> <span class="n">infr</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="n">q</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">factor</span><span class="o">.</span><span class="n">mean</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">post_z_mean</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">post_z_mean</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s">'.'</span><span class="p">)</span>
</code></pre></div></div> <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1a1fc3e668&gt;]
</code></pre></div></div> <p><img src="/mxfusion/images/ppca_tutorial_39_1.png" alt="png" /></p> </div> </article> <div class="row"> </div> <div class="row"> <ul class="pager"> <li><a class="next" href="/mxfusion/pilco/">&laquo; PILCO Pendulum</a></li> </ul> </div> </div> <div class="col-md-4 mt20"> <div class="post-img"> <img width="600" src="/mxfusion/images/ppca_tutorial_39_1.png" alt="PPCA Tutorial"> </div> <div class="mt10 recent"> <h2>Also in MXFusion</h2> <ul> <li> <p><a href="/mxfusion/pilco/">PILCO Pendulum</a></p> </li> </ul> </div> </div> </div> </div> </div> <footer> <div class="container"> <div class="row p20"> <div class="col-md-4 text-center mt25"> </div> </div> </div> </footer> <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script> <script src="/mxfusion/js/bootstrap.min.js"></script> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> </body> </html>
