{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'\n",
    "from mxfusion.common import config\n",
    "config.DEFAULT_DTYPE = 'float64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Setup\n",
    "\n",
    "RL\n",
    "\n",
    "Experiments/simulations expensive (computationally expensive or physically limited / expensive) (robot motor example)\n",
    "-> Model of the physical world -> Data efficiency with experimental/simulated data\n",
    "\n",
    "Model wants to know what parts of the world it doesn't know. -> GPs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pendulum-v0')\n",
    "action_dim = 1\n",
    "obs_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "INITIAL_GYM_STATE = [np.pi, 1.]\n",
    "INITIAL_STATE = [np.cos(INITIAL_GYM_STATE[0]), np.sin(INITIAL_GYM_STATE[0]), INITIAL_GYM_STATE[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_one_episode(env, policy, initial_state=None, max_steps=200, verbose=False, render=False):\n",
    "    \"\"\"\n",
    "    Drives an episode of the OpenAI gym environment using the policy to decide next actions.\n",
    "    \"\"\"\n",
    "    observation = env.reset()\n",
    "    if initial_state is not None:\n",
    "        env.env.state = initial_state\n",
    "        observation = env.env._get_obs()\n",
    "    env._max_episode_steps = max_steps\n",
    "    step_idx = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_actions = []\n",
    "    all_observations = [observation]\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        if verbose:\n",
    "            print(observation)\n",
    "        action = policy(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        all_observations.append(observation)\n",
    "        all_actions.append(action)\n",
    "        total_reward += reward\n",
    "        step_idx += 1\n",
    "        if done or step_idx>=max_steps-1:\n",
    "            print(\"Episode finished after {} timesteps because {}\".format(step_idx+1, \"'done' reached\" if done else \"Max timesteps reached\"))\n",
    "            break\n",
    "    return total_reward, np.array(all_observations, dtype=np.float64,), np.array(all_actions, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion import Model, Variable\n",
    "from mxfusion.components.variables import PositiveTransformation\n",
    "from mxfusion.components.distributions.gp.kernels import RBF\n",
    "from mxfusion.modules.gp_modules import SparseGPRegression, SparseGPRegressionSamplingPrediction\n",
    "from mxfusion.modules.gp_modules import GPRegression, GPRegressionSamplingPrediction\n",
    "import mxnet as mx\n",
    "from mxfusion.inference import GradBasedInference, MAP\n",
    "\n",
    "def prepare_data(state_list, action_list, win_in):\n",
    "    \"\"\"\n",
    "    Prepares a list of states and a list of actions as inputs to the Gaussian Process for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    \n",
    "    for state_array, action_array in zip(state_list, action_list):\n",
    "        # the state and action array shape should be aligned.\n",
    "        assert state_array.shape[0]-1 == action_array.shape[0]\n",
    "        \n",
    "        for i in range(state_array.shape[0]-win_in):\n",
    "            Y_list.append(state_array[i+win_in:i+win_in+1])\n",
    "            X_list.append(np.hstack([state_array[i:i+win_in].flatten(), action_array[i:i+win_in].flatten()]))\n",
    "    X = np.vstack(X_list)\n",
    "    Y = np.vstack(Y_list)\n",
    "    return X, Y\n",
    "\n",
    "def fit_model(state_list, action_list, win_in, verbose=True):\n",
    "    \"\"\"\n",
    "    Fits a Gaussian Process model to the state / action pairs passed in. \n",
    "    This creates a model of the environment which is used during\n",
    "    policy optimization instead of querying the environment directly.\n",
    "    \n",
    "    See mxfusion.gp_modules for additional types of GP models to fit,\n",
    "    including Sparse GP and Stochastic Varitional Inference Sparse GP.\n",
    "    \"\"\"\n",
    "    X, Y = prepare_data(state_list, action_list, win_in)\n",
    "\n",
    "    m = Model()\n",
    "    m.N = Variable()\n",
    "    m.X = Variable(shape=(m.N, X.shape[-1]))\n",
    "    m.noise_var = Variable(shape=(1,), transformation=PositiveTransformation(), initial_value=0.01)\n",
    "    m.kernel = RBF(input_dim=X.shape[-1], variance=1, lengthscale=1, ARD=True)\n",
    "    m.Y = SparseGPRegression.define_variable(X=m.X, kernel=m.kernel,\n",
    "                                             inducing_num = 50,\n",
    "                                             noise_var=m.noise_var, shape=(m.N, Y.shape[-1]))\n",
    "    \n",
    "    gp = m.Y.factor\n",
    "    gp.attach_prediction_algorithms(targets=gp.output_names, conditionals=gp.input_names,\n",
    "                algorithm=SparseGPRegressionSamplingPrediction(\n",
    "                    gp._module_graph,\n",
    "                    gp._extra_graphs[0],\n",
    "                    [gp._module_graph.X],\n",
    "                    jitter = 1e-6,), \n",
    "                    alg_name='gp_predict')\n",
    "\n",
    "    infr = GradBasedInference(inference_algorithm=MAP(model=m, observed=[m.X, m.Y]))\n",
    "    infr.run(X=mx.nd.array(X, dtype='float64'), Y=mx.nd.array(Y, dtype='float64'),\n",
    "             max_iter=1000, learning_rate=0.5, verbose=verbose)\n",
    "    return m, infr, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import GradTransferInference, PILCOAlgorithm, BatchInferenceLoop\n",
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon.parameter import ParameterDict\n",
    "\n",
    "def optimize_policy(policy, cost_func, model, infr, model_data_X, model_data_Y,\n",
    "                    initial_state_generator, num_grad_steps,\n",
    "                    learning_rate=1e-2, num_time_steps=100, \n",
    "                    num_samples=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Takes as primary inputs a policy, cost function, and trained model.\n",
    "    Optimizes the policy for num_grad_steps number of iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    from mxfusion.inference.pilco_alg import PolicyUpdateGPParametricApprox\n",
    "    mb_alg = PolicyUpdateGPParametricApprox(model=model, \n",
    "                            observed=[model.X, model.Y], \n",
    "                            cost_function=cost_func, \n",
    "                            policy=policy, \n",
    "                            n_time_steps=num_time_steps,\n",
    "                            initial_state_generator=initial_state_generator,\n",
    "                            num_samples=num_samples)\n",
    "    \n",
    "    train_params = policy.collect_params() if isinstance(policy, Block) else ParameterDict()\n",
    "    infr_pred = GradTransferInference(mb_alg, \n",
    "                                  infr_params=infr.params, train_params=train_params)\n",
    "    infr_pred.run(max_iter=num_grad_steps, \n",
    "                  X=mx.nd.array(model_data_X, dtype='float64'),\n",
    "                  Y=mx.nd.array(model_data_Y, dtype='float64'),\n",
    "                  verbose=verbose,\n",
    "                  learning_rate=learning_rate)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruePendulumCostFunction(mx.gluon.HybridBlock):\n",
    "    \"\"\"\n",
    "    The goal is to get the pendulum upright and stable as quickly as possible.\n",
    "    Taken from the code for Pendulum.\n",
    "    \"\"\"\n",
    "    def hybrid_forward(self, F, state, action):\n",
    "        \"\"\"\n",
    "        :param state: [np.cos(theta), np.sin(theta), ~ momentum(theta)]\n",
    "        a -> 0 when pendulum is upright, largest when pendulum is hanging down completely.\n",
    "        b -> penalty for taking action\n",
    "        c -> penalty for pendulum momentum\n",
    "        \"\"\"\n",
    "        a_scale = 2.\n",
    "        b_scale = .001\n",
    "        c_scale = .1\n",
    "        a = F.sum(a_scale * (state[:,:,0:1] -1) ** 2, axis=-1)\n",
    "        b = F.sum(b_scale * action ** 2, axis=-1)\n",
    "        c = F.sum(c_scale * state[:,:,2:3] ** 2, axis=-1)\n",
    "        return (a + c + b)\n",
    "    \n",
    "cost = TruePendulumCostFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Initial State Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state_generator(num_initial_states):\n",
    "    \"\"\"\n",
    "    Starts from valid states by drawing theta and momentum\n",
    "    then computing np.cos(theta) and np.sin(theta) for state[0:2].s\n",
    "    \"\"\"\n",
    "    theta = mx.nd.random.uniform(low=0., high=2*np.pi, shape=(num_initial_states, 1), dtype='float64')\n",
    "    thdot = mx.nd.random.uniform(low=-8, high=8, shape=(num_initial_states, 1), dtype='float64')\n",
    "    return mx.nd.concat(mx.nd.cos(theta), mx.nd.sin(theta),thdot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.nn import HybridSequential\n",
    "from mxnet.gluon import HybridBlock\n",
    "\n",
    "def random_policy(state):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "class MultiplyByTwo(HybridBlock):\n",
    "    def hybrid_forward(self, F, X):\n",
    "        return X * 2\n",
    "\n",
    "\n",
    "class EpsilonGreed(HybridBlock):\n",
    "    \"\"\"\n",
    "    With epsilon probability, chooses a random action instead of the computed one.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon, bounds, **kwargs):\n",
    "        super(EpsilonGreed, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.bounds = bounds\n",
    "    \n",
    "    def hybrid_forward(self, F, X):\n",
    "        p = F.random.uniform()\n",
    "        if p >= self.epsilon:\n",
    "            return X\n",
    "        else:\n",
    "            a = F.random.uniform(low=self.bounds[0],\n",
    "                                 high=self.bounds[1],\n",
    "                                 shape=X.shape,\n",
    "                                 dtype=X.dtype)\n",
    "            return a\n",
    "\n",
    "def make_nonlinear_policy(dense_units, epsilon=None):\n",
    "    \"\"\"\n",
    "    Make a simple neural network with one hidden layer.\n",
    "    If epsilon is passed in it will be an epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    policy = HybridSequential()\n",
    "    policy.add(mx.gluon.nn.Dense(dense_units, in_units=obs_dim, dtype='float64', activation='relu'))\n",
    "    policy.add(mx.gluon.nn.Dense(1, in_units=dense_units, dtype='float64', activation='tanh'))\n",
    "    policy.add(MultiplyByTwo())\n",
    "    if epsilon is not None:\n",
    "        policy.add(EpsilonGreed(epsilon, (-2,2)))\n",
    "    return policy\n",
    "\n",
    "epsilon = 0.2\n",
    "dense_units = 50\n",
    "policy = make_nonlinear_policy(dense_units, epsilon=epsilon)\n",
    "# policy = mx.gluon.nn.Dense(1, in_units=2, dtype='float64')\n",
    "policy.collect_params().initialize(mx.init.Xavier(magnitude=3.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an episode with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_one_episode(env, random_policy, max_steps=200, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training loop to optimize the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "all_actions = []\n",
    "\n",
    "num_episode = 40 # how many model fit + policy optimization episodes to run\n",
    "num_samples = 20 # how many sample trajectories the policy optimization loop uses\n",
    "num_grad_steps = 50 # how many gradient steps the optimizer takes per episode\n",
    "num_time_steps = 100 # how far to roll out each sample trajectory\n",
    "learning_rate = 1e-3 # learning rate for the policy optimization\n",
    "\n",
    "\"\"\"\n",
    "If true, the first environment run will be driven with a random policy\n",
    "instead of the real policy for better exploration\n",
    "\"\"\"\n",
    "initialize_with_random_policy = False\n",
    "\n",
    "for i_ep in range(num_episode):\n",
    "    print('Start Episode '+str(i_ep+1)+'.')\n",
    "    \n",
    "    # Run an episode and collect data.    \n",
    "    if i_ep==0 and initialize_with_random_policy:\n",
    "        print(\"Using a random policy to drive the real enviroment to start with.\")\n",
    "        policy_func = random_policy       \n",
    "    else:\n",
    "        print(\"Using a learned policy to drive the real enviroment.\")\n",
    "        policy_func = lambda x: policy(mx.nd.expand_dims(mx.nd.array(x, dtype='float64'), axis=0)).asnumpy()[0]\n",
    "    total_reward, states, actions = run_one_episode(env, policy_func, initial_state=INITIAL_GYM_STATE,\n",
    "                                                    max_steps=num_time_steps, render=True)\n",
    "    print(\"Actions:\", actions[:5], actions[-5:])\n",
    "    all_states.append(states)\n",
    "    all_actions.append(actions)\n",
    "    \n",
    "    # Fit a model.\n",
    "    print('Fit the model.')\n",
    "    model, infr, model_data_X, model_data_Y = fit_model(all_states, all_actions, win_in=1, verbose=False)\n",
    "        \n",
    "    # Optimize the policy.\n",
    "    print('Optimize the policy.')\n",
    "    policy = optimize_policy(policy, cost, model,\n",
    "                             infr,\n",
    "                             model_data_X, model_data_Y,\n",
    "                             initial_state_generator,\n",
    "                             num_grad_steps=num_grad_steps, \n",
    "                             num_samples=num_samples,\n",
    "                             learning_rate=learning_rate,\n",
    "                             num_time_steps=num_time_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1 = run_one_episode(env, policy, max_steps=3000, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
