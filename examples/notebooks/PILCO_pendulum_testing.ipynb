{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'\n",
    "from mxfusion.common import config\n",
    "config.DEFAULT_DTYPE = 'float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = 1\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 3\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_one_episode(env, policy, initial_state=None, max_steps=10000, verbose=False, render=False):\n",
    "    observation = env.reset()\n",
    "    if initial_state is not None:\n",
    "        env.env.state = initial_state\n",
    "        observation = env.env._get_obs()\n",
    "    env._max_episode_steps = max_steps\n",
    "    step_idx = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_actions = []\n",
    "    all_observations = [observation]\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        if verbose:\n",
    "            print(observation)\n",
    "        action = policy(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        all_observations.append(observation)\n",
    "        all_actions.append(action)\n",
    "        total_reward += reward\n",
    "        step_idx += 1\n",
    "        if done or step_idx>=max_steps-1:\n",
    "            print(\"Episode finished after {} timesteps because {}\".format(step_idx+1, \"'done' reached\" if done else \"Max timesteps reached\"))\n",
    "            break\n",
    "    return total_reward, np.array(all_observations, dtype=np.float64,), np.array(all_actions, dtype=np.float64)\n",
    "#     return total_reward, all_observations, all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "def spin_policy(state):\n",
    "    return np.array(np.random.uniform(low=-2, high=0.)).reshape((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(state_list, action_list, win_in):\n",
    "    \n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    \n",
    "    for state_array, action_array in zip(state_list, action_list):\n",
    "        # the state and action array shape should be aligned.\n",
    "        assert state_array.shape[0]-1 == action_array.shape[0]\n",
    "        \n",
    "        for i in range(state_array.shape[0]-win_in):\n",
    "            Y_list.append(state_array[i+win_in:i+win_in+1])\n",
    "            X_list.append(np.hstack([state_array[i:i+win_in].flatten(), action_array[i:i+win_in].flatten()]))\n",
    "    X = np.vstack(X_list)\n",
    "    Y = np.vstack(Y_list)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion import Model, Variable\n",
    "from mxfusion.components.variables import PositiveTransformation\n",
    "from mxfusion.components.distributions.gp.kernels import RBF\n",
    "from mxfusion.modules.gp_modules import SparseGPRegression, SparseGPRegressionSamplingPrediction\n",
    "from mxfusion.modules.gp_modules import GPRegression, GPRegressionSamplingPrediction\n",
    "import mxnet as mx\n",
    "from mxfusion.inference import GradBasedInference, MAP\n",
    "\n",
    "    \n",
    "def fit_model_synthetic(input_list, output_list, win_in, verbose=True):\n",
    "#     X, Y = prepare_data(state_list, action_list, win_in)\n",
    "    \n",
    "#     Y_mean = Y.mean()\n",
    "#     Y_std = Y.std()\n",
    "#     Y = (Y-Y_mean)/(Y_std + 1e-10)\n",
    "    X = input_list # [(state, action), ...]\n",
    "    Y = output_list # [next state]\n",
    "\n",
    "    m = Model()\n",
    "    m.N = Variable()\n",
    "    m.X = Variable(shape=(m.N, X.shape[-1]))\n",
    "    m.noise_var = Variable(shape=(1,), transformation=PositiveTransformation(), initial_value=0.01)\n",
    "    m.kernel = RBF(input_dim=X.shape[-1], variance=1, lengthscale=1, ARD=True)\n",
    "    m.Y = GPRegression.define_variable(X=m.X, kernel=m.kernel, noise_var=m.noise_var, shape=(m.N, Y.shape[-1]))\n",
    "    m.Y.factor.gp_log_pdf.jitter = 1e-6\n",
    "    \n",
    "    gp = m.Y.factor\n",
    "    gp.attach_prediction_algorithms(targets=gp.output_names, conditionals=gp.input_names,\n",
    "                algorithm=GPRegressionSamplingPrediction(\n",
    "                    gp._module_graph, gp._extra_graphs[0], [gp._module_graph.X]), \n",
    "                alg_name='gp_predict')\n",
    "\n",
    "    infr = GradBasedInference(inference_algorithm=MAP(model=m, observed=[m.X, m.Y]))\n",
    "    infr.run(X=mx.nd.array(X, dtype='float64'), Y=mx.nd.array(Y, dtype='float64'),\n",
    "             max_iter=100, learning_rate=0.5, verbose=verbose)\n",
    "    return m, infr, X, Y #, Y_mean, Y_std\n",
    "    \n",
    "def fit_model(state_list, action_list, win_in, verbose=False):\n",
    "    X, Y = prepare_data(state_list, action_list, win_in)\n",
    "    \n",
    "#     Y_mean = Y.mean()\n",
    "#     Y_std = Y.std()\n",
    "#     Y = (Y-Y_mean)/(Y_std + 1e-10)\n",
    "\n",
    "    m = Model()\n",
    "    m.N = Variable()\n",
    "    m.X = Variable(shape=(m.N, X.shape[-1]))\n",
    "    m.noise_var = Variable(shape=(1,), transformation=PositiveTransformation(), initial_value=0.01)\n",
    "    m.kernel = RBF(input_dim=X.shape[-1], variance=1, lengthscale=1, ARD=True)\n",
    "    m.Y = GPRegression.define_variable(X=m.X, kernel=m.kernel, noise_var=m.noise_var, shape=(m.N, Y.shape[-1]))\n",
    "    m.Y.factor.gp_log_pdf.jitter = 1e-6\n",
    "    \n",
    "    gp = m.Y.factor\n",
    "    gp.attach_prediction_algorithms(targets=gp.output_names, conditionals=gp.input_names,\n",
    "                algorithm=GPRegressionSamplingPrediction(\n",
    "                    gp._module_graph, gp._extra_graphs[0], [gp._module_graph.X]), \n",
    "                alg_name='gp_predict')\n",
    "\n",
    "    infr = GradBasedInference(inference_algorithm=MAP(model=m, observed=[m.X, m.Y]))\n",
    "    infr.run(X=mx.nd.array(X, dtype='float64'), Y=mx.nd.array(Y, dtype='float64'),\n",
    "             max_iter=100, learning_rate=0.5, verbose=verbose)\n",
    "    return m, infr, X, Y #, Y_mean, Y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "all_actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward, states, actions = run_one_episode(env, spin_policy, max_steps=200, render=True)\n",
    "all_states.append(states)\n",
    "all_actions.append(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, infr, model_train_X, model_train_Y = fit_model(all_states, all_actions, win_in=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import TransferInference, PILCOAlgorithm, BatchInferenceLoop\n",
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon.parameter import ParameterDict\n",
    "\n",
    "def run_policy(policy, cost_func, model, infr, model_data_X, model_data_Y,\n",
    "                    initial_state_generator, num_grad_steps,\n",
    "                    learning_rate=1e-2, num_time_steps=100, \n",
    "                    num_samples=10, verbose=True):\n",
    "    \n",
    "    from mxfusion.inference import GradTransferInference, PILCOAlgorithm, BatchInferenceLoop\n",
    "    from mxfusion.inference.pilco_alg import PolicyUpdateGPParametricApprox\n",
    "    mb_alg = PolicyUpdateGPParametricApprox(model=model, \n",
    "                                 observed=[model.X, model.Y], \n",
    "                                 cost_function=cost_func, \n",
    "                                 policy=policy,\n",
    "                                 n_time_steps=num_time_steps,\n",
    "                                 initial_state_generator=initial_state_generator,\n",
    "                                 num_samples=num_samples)\n",
    "    \n",
    "    train_params = policy.collect_params() if isinstance(policy, Block) else ParameterDict()\n",
    "    infr_pred = TransferInference(mb_alg, \n",
    "                                  infr_params=infr.params)\n",
    "    rewards = infr_pred.run(max_iter=num_grad_steps, \n",
    "                  X=mx.nd.array(model_data_X, dtype='float64'),\n",
    "                  Y=mx.nd.array(model_data_Y, dtype='float64'),\n",
    "                  verbose=verbose,\n",
    "                  learning_rate=learning_rate)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import GradTransferInference, PILCOAlgorithm, BatchInferenceLoop, TransferInference\n",
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon.parameter import ParameterDict\n",
    "\n",
    "def optimize_policy(policy, cost_func, model, infr, model_data_X, model_data_Y,\n",
    "                    initial_state_generator, num_grad_steps,\n",
    "                    learning_rate=1e-2, momentum=0, num_time_steps=100, \n",
    "                    num_samples=10, verbose=True):\n",
    "    \n",
    "    from mxfusion.inference.pilco_alg import PolicyUpdateGPParametricApprox\n",
    "    mb_alg = PolicyUpdateGPParametricApprox(model=model, \n",
    "                                 observed=[model.X, model.Y], \n",
    "                                 cost_function=cost_func, \n",
    "                                 policy=policy, \n",
    "                                 n_time_steps=num_time_steps,\n",
    "                                 initial_state_generator=initial_state_generator,\n",
    "                                 num_samples=num_samples)\n",
    "    \n",
    "    train_params = policy.collect_params() if isinstance(policy, Block) else ParameterDict()\n",
    "    infr_pred = GradTransferInference(mb_alg, \n",
    "                                  infr_params=infr.params, train_params=train_params)\n",
    "    infr_pred.run(max_iter=num_grad_steps, \n",
    "                  X=mx.nd.array(model_data_X, dtype='float64'),\n",
    "                  Y=mx.nd.array(model_data_Y, dtype='float64'),\n",
    "                  verbose=verbose,\n",
    "                  learning_rate=learning_rate, momentum=momentum)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarCostFunction(mx.gluon.HybridBlock):\n",
    "    def hybrid_forward(self, F, state, action):\n",
    "        return F.sum(10*(state[:,:,0:1] - 0.45)**2, axis=-1)\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return (((x+np.pi) % (2*np.pi)) - np.pi)\n",
    "\n",
    "class TruePendulumCostFunction(mx.gluon.HybridBlock):\n",
    "    \"\"\"\n",
    "    Taken from the code. True except for the action penalty\n",
    "    \"\"\"\n",
    "    def hybrid_forward(self, F, state, action):\n",
    "        cos_th, thdot = state[:,:,0], state[:,:,2]\n",
    "        th = mx.nd.arccos(cos_th)\n",
    "        tmp_a = mx.nd.reshape(action, shape=th.shape)\n",
    "        return  - (angle_normalize(th)**2 + .1*thdot**2 + .001*(tmp_a**2))\n",
    "\n",
    "    \n",
    "class SimplePendulumCostFunction(mx.gluon.HybridBlock):\n",
    "    \"\"\"\n",
    "    Taken from the code.\n",
    "    \"\"\"\n",
    "    def hybrid_forward(self, F, state, action):\n",
    "        a_scale = 2. # 2. ~ theta **2\n",
    "        b_scale = .001 # 0.001\n",
    "        c_scale = .1  # 0.1\n",
    "        a = F.sum(a_scale * (state[:,:,0:1] -1) ** 2, axis=-1)\n",
    "        b = F.sum(b_scale * action ** 2, axis=-1)\n",
    "        c = F.sum(c_scale * state[:,:,2:3] ** 2, axis=-1)\n",
    "#         print(F.sum(a),F.sum(b),F.sum(c))\n",
    "        return (a + c + b)\n",
    "    \n",
    "cost = SimplePendulumCostFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = []\n",
    "for i in range(states.shape[1]):\n",
    "    axis = states[:,i]\n",
    "    bounds.append((np.min(axis), np.max(axis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InitialStateGenerator:\n",
    "#     def __init__(self, bounds, dtype='float64'):\n",
    "#         self.bounds = bounds\n",
    "#         self.dtype = dtype\n",
    "#     def __call__(self, num_initial_states):\n",
    "#         states = None\n",
    "#         for mini, maxi in self.bounds:\n",
    "#             b = mx.nd.random.uniform(low=mini, high=maxi, shape=(num_initial_states, 1), dtype=self.dtype)\n",
    "#             if states is None:\n",
    "#                 states = b\n",
    "#             else:\n",
    "#                 states = mx.nd.concat(states, b)\n",
    "#         return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state_generator(num_initial_states):\n",
    "    theta = mx.nd.random.uniform(low=0., high=2*np.pi, shape=(num_initial_states, 1), dtype='float64')\n",
    "    thdot = mx.nd.random.uniform(low=-8, high=8, shape=(num_initial_states, 1), dtype='float64')\n",
    "    return mx.nd.concat(mx.nd.cos(theta), mx.nd.sin(theta),thdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_state_generator = InitialStateGenerator([(-1,1),(-1,1),(-2,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_generator(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full PILCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.nn import HybridSequential\n",
    "from mxnet.gluon import HybridBlock\n",
    "# Initial step\n",
    "\n",
    "class MultiplyByTwo(HybridBlock):\n",
    "    def hybrid_forward(self, F, X):\n",
    "        return X * 2\n",
    "\n",
    "\n",
    "class EpsilonGreed(HybridBlock):\n",
    "    def __init__(self, epsilon, bounds, **kwargs):\n",
    "        super(EpsilonGreed, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "        self.bounds = bounds\n",
    "    \n",
    "    def hybrid_forward(self, F, X):\n",
    "        p = F.random.uniform()\n",
    "        if p >= self.epsilon:\n",
    "            return X\n",
    "        else:\n",
    "            a = F.random.uniform(low=self.bounds[0],\n",
    "                                 high=self.bounds[1],\n",
    "                                 shape=X.shape,\n",
    "                                 dtype=X.dtype)\n",
    "            return a\n",
    "    \n",
    "epsilon = 0.2\n",
    "dense_units = 50\n",
    "policy = HybridSequential()\n",
    "policy.add(mx.gluon.nn.Dense(dense_units, in_units=obs_dim, dtype='float64', activation='relu'))\n",
    "policy.add(mx.gluon.nn.Dense(1, in_units=dense_units, dtype='float64', activation='tanh'))\n",
    "policy.add(MultiplyByTwo())\n",
    "policy.add(EpsilonGreed(epsilon, (-2,2)))\n",
    "\n",
    "# policy = mx.gluon.nn.Dense(1, in_units=2, dtype='float64')\n",
    "policy.collect_params().initialize(mx.init.Xavier(magnitude=3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_set_policy_parameters(policy):\n",
    "    for n, p in policy.collect_params().items():\n",
    "        param = mx.nd.random.uniform(low=-5, high=5, shape=p.shape, dtype='float64')\n",
    "        p.set_data(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_GYM_STATE = [np.pi, 1.]\n",
    "INITIAL_STATE = [np.cos(INITIAL_GYM_STATE[0]), np.sin(INITIAL_GYM_STATE[0]), INITIAL_GYM_STATE[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_state_generator(num_samples):\n",
    "    return mx.nd.array([INITIAL_STATE] * num_samples, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_state_generator(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model fit on manually generated state,action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pair(env, initial_state, action):\n",
    "    env.env.state = initial_state\n",
    "    us_initial = [np.cos(initial_state[0]), np.sin(initial_state[0]), initial_state[1]]\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    return (us_initial, action, observation, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_state_action_pair():\n",
    "    state = mx.nd.random.uniform(low=-2*np.pi, high=2*np.pi, shape=(1,2), dtype='float64')\n",
    "    action = mx.nd.random.uniform(low=-2, high=2, shape=(1,), dtype='float64')\n",
    "    return state.asnumpy()[0], action.asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "all_states = []\n",
    "all_actions = []\n",
    "all_obs = []\n",
    "for i in range(500):\n",
    "    state, action = sample_state_action_pair()\n",
    "    state, action, obs, reward = evaluate_pair(env, state, action)\n",
    "    all_states.append(state)\n",
    "    all_actions.append(action)\n",
    "    all_obs.append(obs)\n",
    "    all_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reward = np.argmax(all_rewards)\n",
    "gym_state = all_states[best_reward]\n",
    "gym_state = [np.arccos(gym_state[0]), gym_state[2]]\n",
    "env.env.state = gym_state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.array(all_states), np.array(all_actions)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(all_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, infr, model_data_X, model_data_Y = fit_model_synthetic(X, Y, win_in=1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import TransferInference, ModulePredictionAlgorithm\n",
    "infr_pred = TransferInference(ModulePredictionAlgorithm(model=model, observed=[model.X], target_variables=[model.Y], num_samples=100), \n",
    "                              infr_params=infr.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = infr_pred.run(X=mx.nd.array(X, dtype='float64'))[0]\n",
    "f_mean, f_var = res[0].asnumpy()[0], res[1].asnumpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(Y - np.mean(res.asnumpy(), axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test reward correlation from state, action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_parameters = []\n",
    "rewards = []\n",
    "all_states = []\n",
    "all_actions = []\n",
    "for i in range(500):\n",
    "    params = sample_and_set_policy_parameters(policy)\n",
    "    policy_parameters.append(params)\n",
    "    policy_func = lambda x: policy(mx.nd.expand_dims(mx.nd.array(x, dtype='float64'), axis=0)).asnumpy()[0]\n",
    "    true_rewards, states, actions = run_one_episode(env, random_policy, initial_state=INITIAL_GYM_STATE, max_steps=100, render=False)\n",
    "    s = mx.nd.array(states[1:])\n",
    "    s = mx.nd.expand_dims(s, axis=1)\n",
    "    a = mx.nd.array(actions)\n",
    "    a = mx.nd.expand_dims(a, axis=1)\n",
    "    \n",
    "    our_rewards = -mx.nd.sum(cost(s, a))\n",
    "    print(\"Paired:\",true_rewards, our_rewards)\n",
    "    rewards.append(np.array([true_rewards, our_rewards.asnumpy()]))\n",
    "    all_states.append(states)\n",
    "    all_actions.append(actions)\n",
    "rewards = np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "plt.plot(rewards[:,0], rewards[:,1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test  reward correlations from full runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_parameters = []\n",
    "rewards = []\n",
    "timesteps = 5\n",
    "for i in range(5):\n",
    "    params = sample_and_set_policy_parameters(policy)\n",
    "    policy_parameters.append(params)\n",
    "    policy_func = lambda x: policy(mx.nd.expand_dims(mx.nd.array(x, dtype='float64'), axis=0)).asnumpy()[0]\n",
    "    true_rewards, states, actions = run_one_episode(env, policy_func,\n",
    "                                                    initial_state=INITIAL_GYM_STATE,\n",
    "                                                    max_steps=timesteps, render=False)\n",
    "#     print(\"True states:\", states)\n",
    "    our_costs, _ = run_policy(policy, cost, model, infr, model_data_X, model_data_Y,\n",
    "                    static_state_generator, 100,\n",
    "                    learning_rate=1e-2, num_time_steps=timesteps, \n",
    "                    num_samples=10, verbose=True)\n",
    "    our_costs = -our_costs / 10.\n",
    "    print(\"Rewards: \", true_rewards, our_costs.asnumpy())\n",
    "    rewards.append(np.array([true_rewards, our_costs.asnumpy()]))\n",
    "rewards = np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "plt.plot(rewards[:,0], rewards[:,1], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy[2].epsilon = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "all_actions = []\n",
    "\n",
    "num_episode = 40\n",
    "\n",
    "num_grad_steps = 50\n",
    "num_time_steps = 250\n",
    "learning_rate = 1e-4\n",
    "use_random_policy = False\n",
    "policies = []\n",
    "\n",
    "for i_ep in range(num_episode):\n",
    "    print('Start Episode '+str(i_ep+1)+'.')\n",
    "    \n",
    "    # Run an episode and collect data.    \n",
    "    if i_ep==0 and use_random_policy:\n",
    "        print(\"Using random policy\")\n",
    "        policy_func = random_policy       \n",
    "#         learning_rate = 1e-2\n",
    "    else:\n",
    "        print(\"Using learned policy\")\n",
    "        policy_func = lambda x: policy(mx.nd.expand_dims(mx.nd.array(x, dtype='float64'), axis=0)).asnumpy()[0]\n",
    "#         learning_rate = 1e-2\n",
    "    total_reward, states, actions = run_one_episode(env, policy_func, initial_state=INITIAL_GYM_STATE,\n",
    "                                                    max_steps=num_time_steps, render=True)\n",
    "    print(\"Actions:\", actions[:5], actions[-5:])\n",
    "    all_states.append(states)\n",
    "    all_actions.append(actions)\n",
    "    \n",
    "    # Fit a model.\n",
    "#     print('Fit the model.')\n",
    "#     model, infr, model_data_X, model_data_Y, = fit_model(all_states, all_actions, win_in=1, verbose=False)\n",
    "        \n",
    "    # Optimize the policy.\n",
    "    print('Optimize the policy.')\n",
    "    policy = optimize_policy(policy, cost, model,\n",
    "                             infr,\n",
    "                             model_data_X, model_data_Y,\n",
    "                             initial_state_generator,\n",
    "                             num_grad_steps=num_grad_steps, \n",
    "                             num_samples=num_samples,\n",
    "                             learning_rate=learning_rate,\n",
    "                             num_time_steps=num_time_steps)\n",
    "#     datum = (policy.collect_params()['dense0_weight'].data().asnumpy(),\n",
    "#              policy.collect_params()['dense0_bias'].data().asnumpy())\n",
    "#     policies.append(datum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. update the optimize policy fuction.\n",
    "2. multiple initial states.\n",
    "3. use the real reward function. (https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py)\n",
    "4. visualize the intermediate and final performance of policy.\n",
    "5. Make a notebook to show.\n",
    "6. Try a non-linear policy.\n",
    "7. add epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1 = run_one_episode(env, policy_func, max_steps=3000, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy[0].collect_params()['dense0_weight'].set_data(mx.nd.array([[-2.26582996, -4.80595503]], dtype='float64'))\n",
    "policy[0].collect_params()['dense0_bias'].set_data(mx.nd.array([1.73686036], dtype='float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1 = run_one_episode(env, policy_func, max_steps=10000, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
