{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MXNET_ENGINE_TYPE'] = 'NaiveEngine'\n",
    "from mxfusion.common import config\n",
    "config.DEFAULT_DTYPE = 'float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_one_episode(env, policy, max_steps=None, verbose=False, render=False):\n",
    "    observation = env.reset()\n",
    "    step_idx = 0\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_observations = [np.array(observation)]\n",
    "    all_actions = []\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        if verbose:\n",
    "            print(observation)\n",
    "        action = policy(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        all_observations.append(observation)\n",
    "        all_actions.append(action)\n",
    "        total_reward += reward\n",
    "        step_idx += 1\n",
    "        if done or (max_steps is not None and step_idx>=max_steps-1):\n",
    "            print(\"Episode finished after {} timesteps\".format(step_idx+1))\n",
    "            break\n",
    "    return total_reward, np.array(all_observations, dtype=np.float64), np.array(all_actions, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return [np.random.rand()*2-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(state_list, action_list, win_in):\n",
    "    \n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "    \n",
    "    for state_array, action_array in zip(state_list, action_list):\n",
    "        # the state and action array shape should be aligned.\n",
    "        assert state_array.shape[0]-1 == action_array.shape[0]\n",
    "        \n",
    "        for i in range(state_array.shape[0]-win_in):\n",
    "            Y_list.append(state_array[i+win_in:i+win_in+1])\n",
    "            X_list.append(np.hstack([state_array[i:i+win_in].flatten(), action_array[i:i+win_in].flatten()]))\n",
    "    X = np.vstack(X_list)\n",
    "    Y = np.vstack(Y_list)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion import Model, Variable\n",
    "from mxfusion.components.variables import PositiveTransformation\n",
    "from mxfusion.components.distributions.gp.kernels import RBF\n",
    "from mxfusion.modules.gp_modules import GPRegression, GPRegressionSamplingPrediction\n",
    "import mxnet as mx\n",
    "from mxfusion.inference import GradBasedInference, MAP\n",
    "\n",
    "    \n",
    "def fit_model(state_list, action_list, win_in, verbose=False):\n",
    "    X, Y = prepare_data(state_list, action_list, win_in)\n",
    "    \n",
    "#     Y_mean = Y.mean()\n",
    "#     Y_std = Y.std()\n",
    "#     Y = (Y-Y_mean)/(Y_std + 1e-10)\n",
    "\n",
    "    m = Model()\n",
    "    m.N = Variable()\n",
    "    m.X = Variable(shape=(m.N, X.shape[-1]))\n",
    "    m.noise_var = Variable(shape=(1,), transformation=PositiveTransformation(), initial_value=0.01)\n",
    "    m.kernel = RBF(input_dim=X.shape[-1], variance=1, lengthscale=1, ARD=True)\n",
    "    m.Y = GPRegression.define_variable(X=m.X, kernel=m.kernel, noise_var=m.noise_var, shape=(m.N, Y.shape[-1]))\n",
    "    m.Y.factor.gp_log_pdf.jitter = 1e-6\n",
    "    \n",
    "    gp = m.Y.factor\n",
    "    gp.attach_prediction_algorithms(targets=gp.output_names, conditionals=gp.input_names,\n",
    "                algorithm=GPRegressionSamplingPrediction(\n",
    "                    gp._module_graph, gp._extra_graphs[0], [gp._module_graph.X]), \n",
    "                alg_name='gp_predict')\n",
    "\n",
    "    infr = GradBasedInference(inference_algorithm=MAP(model=m, observed=[m.X, m.Y]))\n",
    "    infr.run(X=mx.nd.array(X, dtype='float64'), Y=mx.nd.array(Y, dtype='float64'),\n",
    "             max_iter=100, learning_rate=0.5, verbose=verbose)\n",
    "    return m, infr #, Y_mean, Y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "all_actons = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward, states, actions = run_one_episode(env, random_policy, max_steps=100)\n",
    "all_states.append(states)\n",
    "all_actons.append(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, infr = fit_model(all_states, all_actons, win_in=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import GradTransferInference, ModelBasedAlgorithm, BatchInferenceLoop\n",
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon.parameter import ParameterDict\n",
    "\n",
    "def optimize_policy(policy, cost_func, model, infr, \n",
    "                    initial_state_generator, num_grad_steps,\n",
    "                    learning_rate=1e-2, num_time_steps=100, \n",
    "                    num_samples=10, verbose=True):\n",
    "    \n",
    "    from mxfusion.inference import GradTransferInference, ModelBasedAlgorithm, BatchInferenceLoop\n",
    "    mb_alg = ModelBasedAlgorithm(model=model, \n",
    "                                 observed=[model.X], \n",
    "                                 cost_function=cost_func, \n",
    "                                 policy=policy, \n",
    "                                 n_time_steps=num_time_steps,\n",
    "                                 initial_state_generator=initial_state_generator,\n",
    "                                 num_samples=num_samples)\n",
    "    \n",
    "    train_params = policy.collect_params() if isinstance(policy, Block) else ParameterDict()\n",
    "    infr_pred = GradTransferInference(mb_alg, \n",
    "                                  infr_params=infr.params, train_params=train_params)\n",
    "    infr_pred.run(max_iter=num_grad_steps, \n",
    "                  X=mx.nd.array(np.zeros((1,3)), dtype='float64'),\n",
    "                  verbose=verbose,\n",
    "                  learning_rate=learning_rate)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction(mx.gluon.HybridBlock):\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.sum(10*(x[:,:,0:1] - 0.45)**2, axis=-1)\n",
    "    \n",
    "cost = CostFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state_generator(num_initial_states):\n",
    "    initial_pos = mx.nd.random.uniform(low=-1.2, high=0.6, shape=(num_initial_states, 1), dtype='float64')\n",
    "    initial_vel = mx.nd.random.uniform(low=-0.07, high=0.07, shape=(num_initial_states, 1), dtype='float64')\n",
    "    return mx.nd.concat(initial_pos, initial_vel, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_generator(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full PILCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.nn import HybridSequential\n",
    "# Initial step\n",
    "\n",
    "cost = CostFunction()\n",
    "policy = HybridSequential()\n",
    "policy.add(mx.gluon.nn.Dense(1, in_units=2, dtype='float64', activation='tanh'))\n",
    "# policy = mx.gluon.nn.Dense(1, in_units=2, dtype='float64')\n",
    "policy.collect_params().initialize(mx.init.Xavier(magnitude=3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = []\n",
    "all_actons = []\n",
    "\n",
    "num_episode = 10\n",
    "num_samples = 10\n",
    "\n",
    "num_grad_steps = 500\n",
    "learning_rate = 1e-1\n",
    "\n",
    "for i_ep in range(num_episode):\n",
    "    print('Start Episode '+str(i_ep+1)+'.')\n",
    "    \n",
    "    # Run an episode and collect data.    \n",
    "    if i_ep==0:\n",
    "        policy_func = random_policy\n",
    "    else:\n",
    "        policy_func = lambda x: policy(mx.nd.expand_dims(mx.nd.array(x, dtype='float64'), axis=0)).asnumpy()[0]\n",
    "    total_reward, states, actions = run_one_episode(env, policy_func, max_steps=100, render=True)\n",
    "    all_states.append(states)\n",
    "    all_actons.append(actions)\n",
    "    \n",
    "    # Fit a model.\n",
    "    print('Fit the model.')\n",
    "    model, infr = fit_model(all_states, all_actons, win_in=1, verbose=False)\n",
    "    \n",
    "    initial_state = mx.nd.array(env.observation_space.sample()[None,:], dtype='float64')\n",
    "    \n",
    "    # Optimize the policy.\n",
    "    print('Optimize the policy.')\n",
    "    policy = optimize_policy(policy, cost, model,\n",
    "                             infr, initial_state_generator,\n",
    "                             num_grad_steps=num_grad_steps, \n",
    "                             num_samples=num_samples,\n",
    "                             learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. update the optimize policy fuction.\n",
    "2. multiple initial states.\n",
    "3. use the real reward function. (https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py)\n",
    "4. visualize the intermediate and final performance of policy.\n",
    "5. Make a notebook to show.\n",
    "6. Try a non-linear policy.\n",
    "7. add epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1 = run_one_episode(env, policy_func, max_steps=10000, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
