{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state_dims = 1\n",
    "n_action_dims = 1\n",
    "n_total_input_dims = n_state_dims + n_action_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500\n",
    "X = np.random.rand(size, n_total_input_dims) * 10\n",
    "# Y = np.random.rand(size, n_state_dims)\n",
    "Y = (X[:,0] + X[:,1])[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.common import config\n",
    "config.DEFAULT_DTYPE = 'float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion import Model, Variable\n",
    "from mxfusion.components.variables import PositiveTransformation\n",
    "from mxfusion.components.distributions.gp.kernels import RBF\n",
    "from mxfusion.modules.gp_modules import GPRegression, GPRegressionSamplingPrediction\n",
    "\n",
    "\n",
    "m = Model()\n",
    "m.N = Variable()\n",
    "m.X = Variable(shape=(m.N, n_total_input_dims))\n",
    "m.noise_var = Variable(shape=(1,), transformation=PositiveTransformation(), initial_value=0.01)\n",
    "m.kernel = RBF(input_dim=n_total_input_dims, variance=1, lengthscale=1)\n",
    "m.Y = GPRegression.define_variable(X=m.X, kernel=m.kernel, noise_var=m.noise_var, shape=(m.N, n_state_dims))\n",
    "\n",
    "gp = m.Y.factor\n",
    "gp.attach_prediction_algorithms(targets=gp.output_names, conditionals=gp.input_names,\n",
    "            algorithm=GPRegressionSamplingPrediction(\n",
    "                gp._module_graph, gp._extra_graphs[0], [gp._module_graph.X]), \n",
    "            alg_name='gp_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxfusion.inference import GradBasedInference, MAP\n",
    "\n",
    "infr = GradBasedInference(inference_algorithm=MAP(model=m, observed=[m.X, m.Y]))\n",
    "infr.run(X=mx.nd.array(X, dtype='float64'), Y=mx.nd.array(Y, dtype='float64'), \n",
    "         max_iter=200, learning_rate=.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_steps = 10\n",
    "initial_state = mx.nd.array([[10.]], dtype='float64') # TODO want a proposal distribution here instead of same.\n",
    "linear_policy = mx.gluon.nn.Dense(1, in_units=n_state_dims, dtype='float64')\n",
    "linear_policy.collect_params().initialize(mx.init.Constant(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction(mx.gluon.HybridBlock):\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.sum((x - 1)**2, axis=-1)\n",
    "    \n",
    "cost = CostFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxfusion.inference import GradTransferInference, ModelBasedAlgorithm, BatchInferenceLoop\n",
    "mb_alg = ModelBasedAlgorithm(model=m, \n",
    "                             observed=[m.X], \n",
    "                             cost_function=cost, \n",
    "                             policy=linear_policy, \n",
    "                             n_time_steps=n_time_steps,\n",
    "                             initial_state=initial_state, num_samples=10)\n",
    "infr_pred = GradTransferInference(mb_alg, \n",
    "                              infr_params=infr.params, train_params=linear_policy.collect_params())\n",
    "infr_pred.run(max_iter=500, X=mx.nd.array(X, dtype='float64'), verbose=True, learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_prints=10\n",
    "# max_iter = 5\n",
    "# verbose=True\n",
    "# trainer = mx.gluon.Trainer(linear_policy.collect_params(),\n",
    "#                            optimizer='adam',\n",
    "#                            optimizer_params={'learning_rate':\n",
    "#                                              1e-1})\n",
    "# iter_step = max(max_iter // n_prints, 1)\n",
    "# for i in range(max_iter):\n",
    "#     with mx.autograd.record():\n",
    "#         loss_for_gradient = infr_pred.run(X=mx.nd.array(X, dtype='float64'), verbose=True)[0]\n",
    "#         loss_for_gradient.backward()\n",
    "#         for p in linear_policy.collect_params().values():\n",
    "#             print(p.grad(), p.data())\n",
    "#     if verbose:\n",
    "#         print('\\rIteration {} loss: {}'.format(i + 1, loss_for_gradient.asscalar()),\n",
    "#               end='')\n",
    "#         if i % iter_step == 0 and i > 0:\n",
    "#             print()\n",
    "#     trainer.step(batch_size=1, ignore_stale_grad=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
